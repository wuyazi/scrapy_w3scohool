from scrapy.selector import HtmlXPathSelector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.contrib.spiders import CrawlSpider
from scrapy.http import Request
from w3school.items import W3SchoolItem

class ContentSpider2Spider(CrawlSpider):
    name = 'content_spider2'
    allowed_domains = ['w3school.com.cn']
    start_urls = [
        'http://www.w3school.com.cn/html/index.asp',
]

    def parse(self,response):
        """from second_urls select title and content"""
        hxs = HtmlXPathSelector(response)
        site = hxs.select('/html/body/div/div[@id="maincontent"]')
        item = W3SchoolItem()
        item['title'] = site.select('h1/text()').extract()[0].encode('utf8')
        item['content'] = ''
        for site_line in site.select('div/descendant::*/text()').extract():
            item['content'] = item['content'] + site_line.encode('utf-8')
        second_urls = hxs.select('/html/body/div/div[@id="navsecond"]/div[@id="course"]/ul//li/a/@href').extract()
        urls = ['http://www.w3school.com.cn'+url for url in second_urls]

        for urle in urls:
            print urle
            try:
                yield Request(url=urle,callback=self.parse)
            except Exception,e:
                print "--------------------------------\n",e

        print item['content']
        yield item
